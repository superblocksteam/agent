"use strict";
// @generated by protoc-gen-es v1.2.0 with parameter "target=ts"
// @generated from file plugins/kafka/v1/plugin.proto (package plugins.kafka.v1, syntax proto3)
/* eslint-disable */
// @ts-nocheck
Object.defineProperty(exports, "__esModule", { value: true });
exports.SuperblocksMetadata = exports.Plugin_Produce = exports.Plugin_Consume_Seek = exports.Plugin_Consume_From = exports.Plugin_Consume = exports.Plugin = exports.Cluster = exports.SASL_Mechanism = exports.SASL = exports.Message = exports.Messages = exports.Topic = exports.Broker = exports.Metadata_Minified = exports.Metadata = exports.Acks = exports.Compression = exports.Operation = void 0;
const protobuf_1 = require("@bufbuild/protobuf");
const plugin_pb_1 = require("../../common/v1/plugin_pb");
/**
 * @generated from enum plugins.kafka.v1.Operation
 */
var Operation;
(function (Operation) {
    /**
     * @generated from enum value: OPERATION_UNSPECIFIED = 0;
     */
    Operation[Operation["UNSPECIFIED"] = 0] = "UNSPECIFIED";
    /**
     * @generated from enum value: OPERATION_CONSUME = 1;
     */
    Operation[Operation["CONSUME"] = 1] = "CONSUME";
    /**
     * @generated from enum value: OPERATION_PRODUCE = 2;
     */
    Operation[Operation["PRODUCE"] = 2] = "PRODUCE";
})(Operation || (exports.Operation = Operation = {}));
// Retrieve enum metadata with: proto3.getEnumType(Operation)
protobuf_1.proto3.util.setEnumType(Operation, "plugins.kafka.v1.Operation", [
    { no: 0, name: "OPERATION_UNSPECIFIED" },
    { no: 1, name: "OPERATION_CONSUME" },
    { no: 2, name: "OPERATION_PRODUCE" },
]);
/**
 * @generated from enum plugins.kafka.v1.Compression
 */
var Compression;
(function (Compression) {
    /**
     * @generated from enum value: COMPRESSION_UNSPECIFIED = 0;
     */
    Compression[Compression["UNSPECIFIED"] = 0] = "UNSPECIFIED";
    /**
     * @generated from enum value: COMPRESSION_GZIP = 1;
     */
    Compression[Compression["GZIP"] = 1] = "GZIP";
    /**
     * @generated from enum value: COMPRESSION_SNAPPY = 2;
     */
    Compression[Compression["SNAPPY"] = 2] = "SNAPPY";
    /**
     * @generated from enum value: COMPRESSION_LZ4 = 3;
     */
    Compression[Compression["LZ4"] = 3] = "LZ4";
    /**
     * @generated from enum value: COMPRESSION_ZSTD = 4;
     */
    Compression[Compression["ZSTD"] = 4] = "ZSTD";
})(Compression || (exports.Compression = Compression = {}));
// Retrieve enum metadata with: proto3.getEnumType(Compression)
protobuf_1.proto3.util.setEnumType(Compression, "plugins.kafka.v1.Compression", [
    { no: 0, name: "COMPRESSION_UNSPECIFIED" },
    { no: 1, name: "COMPRESSION_GZIP" },
    { no: 2, name: "COMPRESSION_SNAPPY" },
    { no: 3, name: "COMPRESSION_LZ4" },
    { no: 4, name: "COMPRESSION_ZSTD" },
]);
/**
 * @generated from enum plugins.kafka.v1.Acks
 */
var Acks;
(function (Acks) {
    /**
     * @generated from enum value: ACKS_UNSPECIFIED = 0;
     */
    Acks[Acks["UNSPECIFIED"] = 0] = "UNSPECIFIED";
    /**
     * @generated from enum value: ACKS_NONE = 1;
     */
    Acks[Acks["NONE"] = 1] = "NONE";
    /**
     * @generated from enum value: ACKS_LEADER = 2;
     */
    Acks[Acks["LEADER"] = 2] = "LEADER";
    /**
     * @generated from enum value: ACKS_ALL = 3;
     */
    Acks[Acks["ALL"] = 3] = "ALL";
})(Acks || (exports.Acks = Acks = {}));
// Retrieve enum metadata with: proto3.getEnumType(Acks)
protobuf_1.proto3.util.setEnumType(Acks, "plugins.kafka.v1.Acks", [
    { no: 0, name: "ACKS_UNSPECIFIED" },
    { no: 1, name: "ACKS_NONE" },
    { no: 2, name: "ACKS_LEADER" },
    { no: 3, name: "ACKS_ALL" },
]);
/**
 * @generated from message plugins.kafka.v1.Metadata
 */
class Metadata extends protobuf_1.Message {
    /**
     * @generated from field: repeated plugins.kafka.v1.Topic topics = 1;
     */
    topics = [];
    /**
     * @generated from field: repeated plugins.kafka.v1.Broker brokers = 2;
     */
    brokers = [];
    constructor(data) {
        super();
        protobuf_1.proto3.util.initPartial(data, this);
    }
    static runtime = protobuf_1.proto3;
    static typeName = "plugins.kafka.v1.Metadata";
    static fields = protobuf_1.proto3.util.newFieldList(() => [
        { no: 1, name: "topics", kind: "message", T: Topic, repeated: true },
        { no: 2, name: "brokers", kind: "message", T: Broker, repeated: true },
    ]);
    static fromBinary(bytes, options) {
        return new Metadata().fromBinary(bytes, options);
    }
    static fromJson(jsonValue, options) {
        return new Metadata().fromJson(jsonValue, options);
    }
    static fromJsonString(jsonString, options) {
        return new Metadata().fromJsonString(jsonString, options);
    }
    static equals(a, b) {
        return protobuf_1.proto3.util.equals(Metadata, a, b);
    }
}
exports.Metadata = Metadata;
/**
 * @generated from message plugins.kafka.v1.Metadata.Minified
 */
class Metadata_Minified extends protobuf_1.Message {
    /**
     * @generated from field: repeated string topics = 1;
     */
    topics = [];
    constructor(data) {
        super();
        protobuf_1.proto3.util.initPartial(data, this);
    }
    static runtime = protobuf_1.proto3;
    static typeName = "plugins.kafka.v1.Metadata.Minified";
    static fields = protobuf_1.proto3.util.newFieldList(() => [
        { no: 1, name: "topics", kind: "scalar", T: 9 /* ScalarType.STRING */, repeated: true },
    ]);
    static fromBinary(bytes, options) {
        return new Metadata_Minified().fromBinary(bytes, options);
    }
    static fromJson(jsonValue, options) {
        return new Metadata_Minified().fromJson(jsonValue, options);
    }
    static fromJsonString(jsonString, options) {
        return new Metadata_Minified().fromJsonString(jsonString, options);
    }
    static equals(a, b) {
        return protobuf_1.proto3.util.equals(Metadata_Minified, a, b);
    }
}
exports.Metadata_Minified = Metadata_Minified;
/**
 * @generated from message plugins.kafka.v1.Broker
 */
class Broker extends protobuf_1.Message {
    /**
     * @generated from field: int32 node_id = 1;
     */
    nodeId = 0;
    /**
     * @generated from field: string address = 2;
     */
    address = "";
    constructor(data) {
        super();
        protobuf_1.proto3.util.initPartial(data, this);
    }
    static runtime = protobuf_1.proto3;
    static typeName = "plugins.kafka.v1.Broker";
    static fields = protobuf_1.proto3.util.newFieldList(() => [
        { no: 1, name: "node_id", kind: "scalar", T: 5 /* ScalarType.INT32 */ },
        { no: 2, name: "address", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    ]);
    static fromBinary(bytes, options) {
        return new Broker().fromBinary(bytes, options);
    }
    static fromJson(jsonValue, options) {
        return new Broker().fromJson(jsonValue, options);
    }
    static fromJsonString(jsonString, options) {
        return new Broker().fromJsonString(jsonString, options);
    }
    static equals(a, b) {
        return protobuf_1.proto3.util.equals(Broker, a, b);
    }
}
exports.Broker = Broker;
/**
 * @generated from message plugins.kafka.v1.Topic
 */
class Topic extends protobuf_1.Message {
    /**
     * @generated from field: string name = 1;
     */
    name = "";
    constructor(data) {
        super();
        protobuf_1.proto3.util.initPartial(data, this);
    }
    static runtime = protobuf_1.proto3;
    static typeName = "plugins.kafka.v1.Topic";
    static fields = protobuf_1.proto3.util.newFieldList(() => [
        { no: 1, name: "name", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    ]);
    static fromBinary(bytes, options) {
        return new Topic().fromBinary(bytes, options);
    }
    static fromJson(jsonValue, options) {
        return new Topic().fromJson(jsonValue, options);
    }
    static fromJsonString(jsonString, options) {
        return new Topic().fromJsonString(jsonString, options);
    }
    static equals(a, b) {
        return protobuf_1.proto3.util.equals(Topic, a, b);
    }
}
exports.Topic = Topic;
/**
 * @generated from message plugins.kafka.v1.Messages
 */
class Messages extends protobuf_1.Message {
    /**
     * @generated from field: repeated plugins.kafka.v1.Message messages = 1;
     */
    messages = [];
    constructor(data) {
        super();
        protobuf_1.proto3.util.initPartial(data, this);
    }
    static runtime = protobuf_1.proto3;
    static typeName = "plugins.kafka.v1.Messages";
    static fields = protobuf_1.proto3.util.newFieldList(() => [
        { no: 1, name: "messages", kind: "message", T: Message, repeated: true },
    ]);
    static fromBinary(bytes, options) {
        return new Messages().fromBinary(bytes, options);
    }
    static fromJson(jsonValue, options) {
        return new Messages().fromJson(jsonValue, options);
    }
    static fromJsonString(jsonString, options) {
        return new Messages().fromJsonString(jsonString, options);
    }
    static equals(a, b) {
        return protobuf_1.proto3.util.equals(Messages, a, b);
    }
}
exports.Messages = Messages;
/**
 * @generated from message plugins.kafka.v1.Message
 */
class Message extends protobuf_1.Message {
    /**
     * @generated from field: string topic = 1;
     */
    topic = "";
    /**
     * @generated from field: int32 partition = 2;
     */
    partition = 0;
    /**
     * @generated from field: int32 offset = 4;
     */
    offset = 0;
    /**
     * NOTE(frank): Need to use google.protobuf.Timestamp here but our json schema library doesn't support bigint.
     * Because of this, we can't use the google.protobuf.Timestamp type OR int64..... Since int32 isn't big enough
     * we have to use a string... // rant over.
     *
     * @generated from field: optional string timestamp = 3;
     */
    timestamp;
    /**
     * @generated from field: optional google.protobuf.Value key = 5;
     */
    key;
    /**
     * @generated from field: optional google.protobuf.Value value = 6;
     */
    value;
    /**
     * NOTE(frank): We could use int64 but some Kafka clients (notably the one we're using) only supports int32.
     *
     * @generated from field: int32 length = 7;
     */
    length = 0;
    /**
     * NOTE(frank): Protobuf doesn't have an int8 type.
     *
     * @generated from field: int32 attributes = 8;
     */
    attributes = 0;
    /**
     * @generated from field: map<string, string> headers = 9;
     */
    headers = {};
    constructor(data) {
        super();
        protobuf_1.proto3.util.initPartial(data, this);
    }
    static runtime = protobuf_1.proto3;
    static typeName = "plugins.kafka.v1.Message";
    static fields = protobuf_1.proto3.util.newFieldList(() => [
        { no: 1, name: "topic", kind: "scalar", T: 9 /* ScalarType.STRING */ },
        { no: 2, name: "partition", kind: "scalar", T: 5 /* ScalarType.INT32 */ },
        { no: 4, name: "offset", kind: "scalar", T: 5 /* ScalarType.INT32 */ },
        { no: 3, name: "timestamp", kind: "scalar", T: 9 /* ScalarType.STRING */, opt: true },
        { no: 5, name: "key", kind: "message", T: protobuf_1.Value, opt: true },
        { no: 6, name: "value", kind: "message", T: protobuf_1.Value, opt: true },
        { no: 7, name: "length", kind: "scalar", T: 5 /* ScalarType.INT32 */ },
        { no: 8, name: "attributes", kind: "scalar", T: 5 /* ScalarType.INT32 */ },
        { no: 9, name: "headers", kind: "map", K: 9 /* ScalarType.STRING */, V: { kind: "scalar", T: 9 /* ScalarType.STRING */ } },
    ]);
    static fromBinary(bytes, options) {
        return new Message().fromBinary(bytes, options);
    }
    static fromJson(jsonValue, options) {
        return new Message().fromJson(jsonValue, options);
    }
    static fromJsonString(jsonString, options) {
        return new Message().fromJsonString(jsonString, options);
    }
    static equals(a, b) {
        return protobuf_1.proto3.util.equals(Message, a, b);
    }
}
exports.Message = Message;
/**
 * @generated from message plugins.kafka.v1.SASL
 */
class SASL extends protobuf_1.Message {
    /**
     * @generated from field: plugins.kafka.v1.SASL.Mechanism mechanism = 1;
     */
    mechanism = SASL_Mechanism.UNSPECIFIED;
    /**
     * non-aws fields
     *
     * @generated from field: optional string username = 2;
     */
    username;
    /**
     * @generated from field: optional string password = 3;
     */
    password;
    /**
     * aws fields
     *
     * @generated from field: optional string access_key_id = 4;
     */
    accessKeyId;
    /**
     * @generated from field: optional string secret_key = 5;
     */
    secretKey;
    /**
     * @generated from field: optional string session_token = 6;
     */
    sessionToken;
    /**
     * @generated from field: optional string authorization_identity = 7;
     */
    authorizationIdentity;
    constructor(data) {
        super();
        protobuf_1.proto3.util.initPartial(data, this);
    }
    static runtime = protobuf_1.proto3;
    static typeName = "plugins.kafka.v1.SASL";
    static fields = protobuf_1.proto3.util.newFieldList(() => [
        { no: 1, name: "mechanism", kind: "enum", T: protobuf_1.proto3.getEnumType(SASL_Mechanism) },
        { no: 2, name: "username", kind: "scalar", T: 9 /* ScalarType.STRING */, opt: true },
        { no: 3, name: "password", kind: "scalar", T: 9 /* ScalarType.STRING */, opt: true },
        { no: 4, name: "access_key_id", kind: "scalar", T: 9 /* ScalarType.STRING */, opt: true },
        { no: 5, name: "secret_key", kind: "scalar", T: 9 /* ScalarType.STRING */, opt: true },
        { no: 6, name: "session_token", kind: "scalar", T: 9 /* ScalarType.STRING */, opt: true },
        { no: 7, name: "authorization_identity", kind: "scalar", T: 9 /* ScalarType.STRING */, opt: true },
    ]);
    static fromBinary(bytes, options) {
        return new SASL().fromBinary(bytes, options);
    }
    static fromJson(jsonValue, options) {
        return new SASL().fromJson(jsonValue, options);
    }
    static fromJsonString(jsonString, options) {
        return new SASL().fromJsonString(jsonString, options);
    }
    static equals(a, b) {
        return protobuf_1.proto3.util.equals(SASL, a, b);
    }
}
exports.SASL = SASL;
/**
 * @generated from enum plugins.kafka.v1.SASL.Mechanism
 */
var SASL_Mechanism;
(function (SASL_Mechanism) {
    /**
     * @generated from enum value: MECHANISM_UNSPECIFIED = 0;
     */
    SASL_Mechanism[SASL_Mechanism["UNSPECIFIED"] = 0] = "UNSPECIFIED";
    /**
     * @generated from enum value: MECHANISM_PLAIN = 1;
     */
    SASL_Mechanism[SASL_Mechanism["PLAIN"] = 1] = "PLAIN";
    /**
     * @generated from enum value: MECHANISM_SCRAM_SHA256 = 2;
     */
    SASL_Mechanism[SASL_Mechanism["SCRAM_SHA256"] = 2] = "SCRAM_SHA256";
    /**
     * @generated from enum value: MECHANISM_SCRAM_SHA512 = 3;
     */
    SASL_Mechanism[SASL_Mechanism["SCRAM_SHA512"] = 3] = "SCRAM_SHA512";
    /**
     * @generated from enum value: MECHANISM_AWS = 4;
     */
    SASL_Mechanism[SASL_Mechanism["AWS"] = 4] = "AWS";
})(SASL_Mechanism || (exports.SASL_Mechanism = SASL_Mechanism = {}));
// Retrieve enum metadata with: proto3.getEnumType(SASL_Mechanism)
protobuf_1.proto3.util.setEnumType(SASL_Mechanism, "plugins.kafka.v1.SASL.Mechanism", [
    { no: 0, name: "MECHANISM_UNSPECIFIED" },
    { no: 1, name: "MECHANISM_PLAIN" },
    { no: 2, name: "MECHANISM_SCRAM_SHA256" },
    { no: 3, name: "MECHANISM_SCRAM_SHA512" },
    { no: 4, name: "MECHANISM_AWS" },
]);
/**
 * @generated from message plugins.kafka.v1.Cluster
 */
class Cluster extends protobuf_1.Message {
    /**
     * NOTE(frank): Due to limitations in our plugin template system, we can't use an array.....
     *
     * @generated from field: string brokers = 1;
     */
    brokers = "";
    /**
     * @generated from field: bool ssl = 2;
     */
    ssl = false;
    /**
     * @generated from field: plugins.kafka.v1.SASL sasl = 3;
     */
    sasl;
    constructor(data) {
        super();
        protobuf_1.proto3.util.initPartial(data, this);
    }
    static runtime = protobuf_1.proto3;
    static typeName = "plugins.kafka.v1.Cluster";
    static fields = protobuf_1.proto3.util.newFieldList(() => [
        { no: 1, name: "brokers", kind: "scalar", T: 9 /* ScalarType.STRING */ },
        { no: 2, name: "ssl", kind: "scalar", T: 8 /* ScalarType.BOOL */ },
        { no: 3, name: "sasl", kind: "message", T: SASL },
    ]);
    static fromBinary(bytes, options) {
        return new Cluster().fromBinary(bytes, options);
    }
    static fromJson(jsonValue, options) {
        return new Cluster().fromJson(jsonValue, options);
    }
    static fromJsonString(jsonString, options) {
        return new Cluster().fromJsonString(jsonString, options);
    }
    static equals(a, b) {
        return protobuf_1.proto3.util.equals(Cluster, a, b);
    }
}
exports.Cluster = Cluster;
/**
 * NOTE(frank): Since it's Kafka, there's a zillion options. We'll start with the basics for now.
 *
 * @generated from message plugins.kafka.v1.Plugin
 */
class Plugin extends protobuf_1.Message {
    /**
     * @generated from field: optional string name = 1;
     */
    name;
    /**
     * @generated from field: plugins.kafka.v1.Operation operation = 2;
     */
    operation = Operation.UNSPECIFIED;
    /**
     * @generated from field: plugins.kafka.v1.Plugin.Produce produce = 3;
     */
    produce;
    /**
     * @generated from field: plugins.kafka.v1.Plugin.Consume consume = 4;
     */
    consume;
    /**
     * @generated from field: plugins.kafka.v1.Cluster cluster = 5;
     */
    cluster;
    /**
     * DEPRECATED
     *
     * @generated from field: plugins.kafka.v1.SuperblocksMetadata superblocksMetadata = 6;
     */
    superblocksMetadata;
    /**
     * @generated from field: optional plugins.common.v1.DynamicWorkflowConfiguration dynamic_workflow_configuration = 7;
     */
    dynamicWorkflowConfiguration;
    constructor(data) {
        super();
        protobuf_1.proto3.util.initPartial(data, this);
    }
    static runtime = protobuf_1.proto3;
    static typeName = "plugins.kafka.v1.Plugin";
    static fields = protobuf_1.proto3.util.newFieldList(() => [
        { no: 1, name: "name", kind: "scalar", T: 9 /* ScalarType.STRING */, opt: true },
        { no: 2, name: "operation", kind: "enum", T: protobuf_1.proto3.getEnumType(Operation) },
        { no: 3, name: "produce", kind: "message", T: Plugin_Produce },
        { no: 4, name: "consume", kind: "message", T: Plugin_Consume },
        { no: 5, name: "cluster", kind: "message", T: Cluster },
        { no: 6, name: "superblocksMetadata", kind: "message", T: SuperblocksMetadata },
        { no: 7, name: "dynamic_workflow_configuration", kind: "message", T: plugin_pb_1.DynamicWorkflowConfiguration, opt: true },
    ]);
    static fromBinary(bytes, options) {
        return new Plugin().fromBinary(bytes, options);
    }
    static fromJson(jsonValue, options) {
        return new Plugin().fromJson(jsonValue, options);
    }
    static fromJsonString(jsonString, options) {
        return new Plugin().fromJsonString(jsonString, options);
    }
    static equals(a, b) {
        return protobuf_1.proto3.util.equals(Plugin, a, b);
    }
}
exports.Plugin = Plugin;
/**
 * @generated from message plugins.kafka.v1.Plugin.Consume
 */
class Plugin_Consume extends protobuf_1.Message {
    /**
     * @generated from field: plugins.kafka.v1.Plugin.Consume.From from = 1;
     */
    from = Plugin_Consume_From.UNSPECIFIED;
    /**
     * NOTE(frank): SMH. Because our form template system if VERY limited,
     * there no way to send an array to the backend if we take in one topic in the UI.
     *
     * @generated from field: string topic = 2;
     */
    topic = "";
    /**
     * @generated from field: optional string group_id = 3;
     */
    groupId;
    /**
     * @generated from field: optional string client_id = 4;
     */
    clientId;
    /**
     * NOTE(frank): Another instance of template system limitations...
     *
     * @generated from field: plugins.kafka.v1.Plugin.Consume.Seek seek = 5;
     */
    seek;
    /**
     * @generated from field: bool read_uncommitted = 6;
     */
    readUncommitted = false;
    constructor(data) {
        super();
        protobuf_1.proto3.util.initPartial(data, this);
    }
    static runtime = protobuf_1.proto3;
    static typeName = "plugins.kafka.v1.Plugin.Consume";
    static fields = protobuf_1.proto3.util.newFieldList(() => [
        { no: 1, name: "from", kind: "enum", T: protobuf_1.proto3.getEnumType(Plugin_Consume_From) },
        { no: 2, name: "topic", kind: "scalar", T: 9 /* ScalarType.STRING */ },
        { no: 3, name: "group_id", kind: "scalar", T: 9 /* ScalarType.STRING */, opt: true },
        { no: 4, name: "client_id", kind: "scalar", T: 9 /* ScalarType.STRING */, opt: true },
        { no: 5, name: "seek", kind: "message", T: Plugin_Consume_Seek },
        { no: 6, name: "read_uncommitted", kind: "scalar", T: 8 /* ScalarType.BOOL */ },
    ]);
    static fromBinary(bytes, options) {
        return new Plugin_Consume().fromBinary(bytes, options);
    }
    static fromJson(jsonValue, options) {
        return new Plugin_Consume().fromJson(jsonValue, options);
    }
    static fromJsonString(jsonString, options) {
        return new Plugin_Consume().fromJsonString(jsonString, options);
    }
    static equals(a, b) {
        return protobuf_1.proto3.util.equals(Plugin_Consume, a, b);
    }
}
exports.Plugin_Consume = Plugin_Consume;
/**
 * @generated from enum plugins.kafka.v1.Plugin.Consume.From
 */
var Plugin_Consume_From;
(function (Plugin_Consume_From) {
    /**
     * @generated from enum value: FROM_UNSPECIFIED = 0;
     */
    Plugin_Consume_From[Plugin_Consume_From["UNSPECIFIED"] = 0] = "UNSPECIFIED";
    /**
     * @generated from enum value: FROM_BEGINNING = 1;
     */
    Plugin_Consume_From[Plugin_Consume_From["BEGINNING"] = 1] = "BEGINNING";
    /**
     * @generated from enum value: FROM_LATEST = 2;
     */
    Plugin_Consume_From[Plugin_Consume_From["LATEST"] = 2] = "LATEST";
    /**
     * @generated from enum value: FROM_SEEK = 3;
     */
    Plugin_Consume_From[Plugin_Consume_From["SEEK"] = 3] = "SEEK";
})(Plugin_Consume_From || (exports.Plugin_Consume_From = Plugin_Consume_From = {}));
// Retrieve enum metadata with: proto3.getEnumType(Plugin_Consume_From)
protobuf_1.proto3.util.setEnumType(Plugin_Consume_From, "plugins.kafka.v1.Plugin.Consume.From", [
    { no: 0, name: "FROM_UNSPECIFIED" },
    { no: 1, name: "FROM_BEGINNING" },
    { no: 2, name: "FROM_LATEST" },
    { no: 3, name: "FROM_SEEK" },
]);
/**
 * @generated from message plugins.kafka.v1.Plugin.Consume.Seek
 */
class Plugin_Consume_Seek extends protobuf_1.Message {
    /**
     * @generated from field: string topic = 1;
     */
    topic = "";
    /**
     * @generated from field: int32 offset = 2;
     */
    offset = 0;
    /**
     * @generated from field: int32 partition = 3;
     */
    partition = 0;
    constructor(data) {
        super();
        protobuf_1.proto3.util.initPartial(data, this);
    }
    static runtime = protobuf_1.proto3;
    static typeName = "plugins.kafka.v1.Plugin.Consume.Seek";
    static fields = protobuf_1.proto3.util.newFieldList(() => [
        { no: 1, name: "topic", kind: "scalar", T: 9 /* ScalarType.STRING */ },
        { no: 2, name: "offset", kind: "scalar", T: 5 /* ScalarType.INT32 */ },
        { no: 3, name: "partition", kind: "scalar", T: 5 /* ScalarType.INT32 */ },
    ]);
    static fromBinary(bytes, options) {
        return new Plugin_Consume_Seek().fromBinary(bytes, options);
    }
    static fromJson(jsonValue, options) {
        return new Plugin_Consume_Seek().fromJson(jsonValue, options);
    }
    static fromJsonString(jsonString, options) {
        return new Plugin_Consume_Seek().fromJsonString(jsonString, options);
    }
    static equals(a, b) {
        return protobuf_1.proto3.util.equals(Plugin_Consume_Seek, a, b);
    }
}
exports.Plugin_Consume_Seek = Plugin_Consume_Seek;
/**
 * @generated from message plugins.kafka.v1.Plugin.Produce
 */
class Plugin_Produce extends protobuf_1.Message {
    /**
     * @generated from field: plugins.kafka.v1.Acks acks = 1;
     */
    acks = Acks.UNSPECIFIED;
    /**
     * @generated from field: optional string client_id = 2;
     */
    clientId;
    /**
     * @generated from field: optional int32 timeout = 3;
     */
    timeout;
    /**
     * @generated from field: optional plugins.kafka.v1.Compression compression = 4;
     */
    compression;
    /**
     * @generated from field: optional string transaction_id = 5;
     */
    transactionId;
    /**
     * @generated from field: bool auto_create_topic = 6;
     */
    autoCreateTopic = false;
    /**
     * @generated from field: bool idempotent = 7;
     */
    idempotent = false;
    /**
     * @generated from field: bool transaction = 8;
     */
    transaction = false;
    /**
     * @generated from field: string messages = 9;
     */
    messages = "";
    constructor(data) {
        super();
        protobuf_1.proto3.util.initPartial(data, this);
    }
    static runtime = protobuf_1.proto3;
    static typeName = "plugins.kafka.v1.Plugin.Produce";
    static fields = protobuf_1.proto3.util.newFieldList(() => [
        { no: 1, name: "acks", kind: "enum", T: protobuf_1.proto3.getEnumType(Acks) },
        { no: 2, name: "client_id", kind: "scalar", T: 9 /* ScalarType.STRING */, opt: true },
        { no: 3, name: "timeout", kind: "scalar", T: 5 /* ScalarType.INT32 */, opt: true },
        { no: 4, name: "compression", kind: "enum", T: protobuf_1.proto3.getEnumType(Compression), opt: true },
        { no: 5, name: "transaction_id", kind: "scalar", T: 9 /* ScalarType.STRING */, opt: true },
        { no: 6, name: "auto_create_topic", kind: "scalar", T: 8 /* ScalarType.BOOL */ },
        { no: 7, name: "idempotent", kind: "scalar", T: 8 /* ScalarType.BOOL */ },
        { no: 8, name: "transaction", kind: "scalar", T: 8 /* ScalarType.BOOL */ },
        { no: 9, name: "messages", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    ]);
    static fromBinary(bytes, options) {
        return new Plugin_Produce().fromBinary(bytes, options);
    }
    static fromJson(jsonValue, options) {
        return new Plugin_Produce().fromJson(jsonValue, options);
    }
    static fromJsonString(jsonString, options) {
        return new Plugin_Produce().fromJsonString(jsonString, options);
    }
    static equals(a, b) {
        return protobuf_1.proto3.util.equals(Plugin_Produce, a, b);
    }
}
exports.Plugin_Produce = Plugin_Produce;
/**
 * DEPRECATED
 *
 * @generated from message plugins.kafka.v1.SuperblocksMetadata
 */
class SuperblocksMetadata extends protobuf_1.Message {
    /**
     * @generated from field: optional string plugin_version = 1;
     */
    pluginVersion;
    /**
     * @generated from field: optional string synced_from_profile_id = 2;
     */
    syncedFromProfileId;
    constructor(data) {
        super();
        protobuf_1.proto3.util.initPartial(data, this);
    }
    static runtime = protobuf_1.proto3;
    static typeName = "plugins.kafka.v1.SuperblocksMetadata";
    static fields = protobuf_1.proto3.util.newFieldList(() => [
        { no: 1, name: "plugin_version", kind: "scalar", T: 9 /* ScalarType.STRING */, opt: true },
        { no: 2, name: "synced_from_profile_id", kind: "scalar", T: 9 /* ScalarType.STRING */, opt: true },
    ]);
    static fromBinary(bytes, options) {
        return new SuperblocksMetadata().fromBinary(bytes, options);
    }
    static fromJson(jsonValue, options) {
        return new SuperblocksMetadata().fromJson(jsonValue, options);
    }
    static fromJsonString(jsonString, options) {
        return new SuperblocksMetadata().fromJsonString(jsonString, options);
    }
    static equals(a, b) {
        return protobuf_1.proto3.util.equals(SuperblocksMetadata, a, b);
    }
}
exports.SuperblocksMetadata = SuperblocksMetadata;
//# sourceMappingURL=plugin_pb.map