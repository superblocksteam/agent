{{- range $k, $v := .Values.fleets }}
{{- $language := $v.language | required "language is required (python or javascript)" }}
{{- $sandboxConfig := index $.Values.sandbox $language }}
{{- $podAnnotations := $v.podAnnotations | default $.Values.podAnnotations }}
---
apiVersion: keda.sh/v1alpha1
kind: ScaledJob
metadata:
  name: {{ $k }}
  labels:
    {{- include "ephemeral.worker.labels" $v | nindent 4 }}
spec:
  pollingInterval: {{ $v.keda.pollingInterval | default $.Values.keda.pollingInterval }}
  successfulJobsHistoryLimit: {{ $v.keda.successfulJobsHistoryLimit | default $.Values.keda.successfulJobsHistoryLimit }}
  failedJobsHistoryLimit: {{ $v.keda.failedJobsHistoryLimit | default $.Values.keda.failedJobsHistoryLimit }}
  maxReplicaCount: {{ $v.keda.maxReplicaCount | default $.Values.keda.maxReplicaCount }}
  minReplicaCount: {{ $v.keda.minReplicaCount | default $.Values.keda.minReplicaCount }}
  {{- if or $v.keda.triggers $.Values.keda.triggers }}
  triggers:
    {{- toYaml ($v.keda.triggers | default $.Values.keda.triggers) | nindent 4 }}
  {{- else }}
  triggers:
    {{- range $v.triggerStreams }}
    - type: redis-streams
      metadata:
        host: {{ $.Values.queue.host }}
        port: {{ $.Values.queue.port | quote }}
        enableTLS: {{ $.Values.queue.tls | quote }}
        stream: {{ . }}
        consumerGroup: {{ $v.consumerGroup }}
        activationLagCount: "1"
      authenticationRef:
        name: redis-trigger-auth
    {{- end }}
  {{- end }}
  scalingStrategy:
    {{- toYaml ($v.keda.scalingStrategy | default $.Values.keda.scalingStrategy | default (dict "strategy" "accurate")) | nindent 4 }}
  jobTargetRef:
    parallelism: 1
    completions: 1
    backoffLimit: 0
    ttlSecondsAfterFinished: {{ $v.keda.ttlSecondsAfterFinished | default $.Values.keda.ttlSecondsAfterFinished }}
    template:
      metadata:
        labels:
          {{- include "ephemeral.worker.labels" $v | nindent 10 }}
        {{- with $podAnnotations }}
        annotations:
          {{- toYaml . | nindent 10 }}
        {{- end }}
      spec:
        restartPolicy: Never
        automountServiceAccountToken: false
        {{- if $.Values.runtimeClassName }}
        runtimeClassName: {{ $.Values.runtimeClassName }}
        {{- end }}
        {{- if $.Values.image.credentials }}
        imagePullSecrets:
        - name: ephemeral-worker-docker
        {{- end }}

        # Sidecars run as initContainers with restartPolicy: Always
        initContainers:
        {{- if $.Values.smokescreen.enabled }}
        # iptables init container - blocks direct HTTP/HTTPS from sandbox
        - name: iptables-init
          image: alpine:3.19
          imagePullPolicy: IfNotPresent
          securityContext:
            capabilities:
              add:
              - NET_ADMIN
          command:
          - /bin/sh
          - -c
          - |
            set -ex
            apk add --no-cache iptables

            PROXY_UID=65534    # smokescreen
            TASKMAN_UID=1000   # task-manager

            echo "=== Configuring network isolation ==="

            # Allow localhost (sandbox <-> task-manager gRPC)
            iptables -A OUTPUT -d 127.0.0.0/8 -j ACCEPT

            # Allow DNS (required for name resolution)
            iptables -A OUTPUT -p udp --dport 53 -j ACCEPT

            # Allow established connections (for allowed UIDs)
            iptables -A OUTPUT -m state --state ESTABLISHED,RELATED -j ACCEPT

            # Allow smokescreen full access (required for proxy)
            iptables -A OUTPUT -m owner --uid-owner $PROXY_UID -j ACCEPT

            # Allow task-manager full access (needs Redis, orchestrator, etc.)
            iptables -A OUTPUT -m owner --uid-owner $TASKMAN_UID -j ACCEPT

            {{- if $.Values.network.vpcCIDR }}
            # Block VPC CIDR (configured via helm values)
            echo "Blocking VPC CIDR: {{ $.Values.network.vpcCIDR }}"
            iptables -A OUTPUT -d {{ $.Values.network.vpcCIDR }} -j DROP
            {{- end }}

            {{- if $.Values.network.serviceCIDR }}
            # Block Kubernetes service CIDR
            echo "Blocking Service CIDR: {{ $.Values.network.serviceCIDR }}"
            iptables -A OUTPUT -d {{ $.Values.network.serviceCIDR }} -j DROP
            {{- end }}

            {{- range $.Values.network.additionalBlockedCIDRs }}
            # Block additional CIDR
            echo "Blocking additional CIDR: {{ . }}"
            iptables -A OUTPUT -d {{ . }} -j DROP
            {{- end }}

            {{- if not $.Values.network.vpcCIDR }}
            # No VPC CIDR specified - block all RFC 1918 private ranges (SSRF protection)
            echo "No VPC CIDR specified - blocking all RFC 1918 ranges"
            iptables -A OUTPUT -d 10.0.0.0/8 -j DROP
            iptables -A OUTPUT -d 172.16.0.0/12 -j DROP
            iptables -A OUTPUT -d 192.168.0.0/16 -j DROP
            iptables -A OUTPUT -d 169.254.0.0/16 -j DROP
            iptables -A OUTPUT -d 100.64.0.0/10 -j DROP
            {{- end }}

            # DEFAULT DENY: Block everything else for sandbox
            iptables -A OUTPUT -j DROP

            echo "=== iptables configured ==="
            echo "task-manager (UID $TASKMAN_UID): full access"
            echo "smokescreen (UID $PROXY_UID): full access"
            echo "sandbox: localhost + smokescreen proxy ONLY"
            iptables -L OUTPUT -n -v --line-numbers
        # Smokescreen egress proxy with mTLS certificate-based access control
        - name: smokescreen
          image: "{{ $.Values.smokescreen.image.repository }}:{{ $.Values.smokescreen.image.tag }}"
          imagePullPolicy: {{ $.Values.smokescreen.image.pullPolicy }}
          restartPolicy: Always
          securityContext:
            runAsUser: 65534
            runAsNonRoot: true
          resources:
            {{- toYaml $.Values.smokescreen.resources | nindent 12 }}
          ports:
          - name: proxy
            containerPort: {{ $.Values.smokescreen.port }}
            protocol: TCP
          volumeMounts:
          - name: smokescreen-config
            mountPath: /etc/smokescreen
            readOnly: true
          {{- if $.Values.smokescreen.mtls.enabled }}
          - name: smokescreen-server-cert
            mountPath: /etc/smokescreen/certs/server
            readOnly: true
          - name: smokescreen-ca-cert
            mountPath: /etc/smokescreen/certs/ca
            readOnly: true
          {{- end }}
        {{- end }}
        # Sandbox runs as a sidecar
        - name: {{ $language }}-sandbox
          image: "{{ (($v.sandbox).image).repository | default $sandboxConfig.image.repository }}:{{ (($v.sandbox).image).tag | default $sandboxConfig.image.tag }}"
          imagePullPolicy: {{ (($v.sandbox).image).pullPolicy | default $sandboxConfig.image.pullPolicy }}
          restartPolicy: Always
          resources:
            {{- toYaml (deepCopy ($sandboxConfig.resources | default dict) | merge (($v.sandbox).resources | default dict)) | nindent 12 }}
          env:
          - name: GRPC_PORT
            value: "{{ $sandboxConfig.port }}"
          {{- with $sandboxConfig.extraEnv }}
          {{- toYaml . | nindent 10 }}
          {{- end }}
          {{- with ($v.sandbox).extraEnv }}
          {{- toYaml . | nindent 10 }}
          {{- end }}
          ports:
          - name: grpc
            containerPort: {{ $sandboxConfig.port }}
            protocol: TCP

        # Task Manager is the main container
        containers:
        - name: task-manager
          image: "{{ (($v.taskManager).image).repository | default $.Values.taskManager.image.repository }}:{{ (($v.taskManager).image).tag | default $.Values.taskManager.image.tag }}"
          imagePullPolicy: {{ (($v.taskManager).image).pullPolicy | default $.Values.taskManager.image.pullPolicy }}
          {{- if $.Values.smokescreen.enabled }}
          securityContext:
            runAsUser: 1000  # Must match TASKMAN_UID in iptables-init
            runAsNonRoot: true
          {{- end }}
          resources:
            {{- toYaml (deepCopy ($.Values.taskManager.resources | default dict) | merge (($v.taskManager).resources | default dict)) | nindent 12 }}
          {{- with (($v.taskManager).startupProbe) | default $.Values.taskManager.startupProbe }}
          startupProbe:
            {{- toYaml . | nindent 12 }}
          {{- end }}
          {{- with (($v.taskManager).readinessProbe) | default $.Values.taskManager.readinessProbe }}
          readinessProbe:
            {{- toYaml . | nindent 12 }}
          {{- end }}
          {{- with (($v.taskManager).livenessProbe) | default $.Values.taskManager.livenessProbe }}
          livenessProbe:
            {{- toYaml . | nindent 12 }}
          {{- end }}
          command: ["./task-manager"]
          args:
          # Ephemeral mode - process one job and exit
          - "--worker.ephemeral=true"
          # Health check configuration
          - "--health.file={{ (($v.taskManager).healthFilePath) | default $.Values.taskManager.healthFilePath }}"
          # Logging
          - "--log.level={{ $.Values.observability.logging.level }}"
          # Language
          - "--worker.language={{ $language }}"
          # Sandbox address (localhost since it's a sidecar)
          - "--sandbox.address=localhost:{{ $sandboxConfig.port }}"
          # gRPC configuration
          - "--grpc.port={{ $.Values.grpc.port }}"
          # Worker configuration
          - "--worker.consumer.group={{ $v.consumerGroup }}"
          {{- if $v.streams }}
          - "--worker.stream.keys={{ join "," $v.streams }}"
          {{- else if $.Values.worker.streams }}
          - "--worker.stream.keys={{ join "," $.Values.worker.streams }}"
          {{- else }}
          - "--worker.group={{ $.Values.worker.group }}"
          - "--worker.bucket={{ $v.bucket }}"
          {{- if $v.events }}
          - "--worker.events={{ join "," ($v.events | default $.Values.worker.events) }}"
          {{- else if $.Values.worker.events }}
          - "--worker.events={{ join "," $.Values.worker.events }}"
          {{- end }}
          {{- end }}
          # Redis transport
          - "--transport.redis.host={{ $.Values.queue.host }}"
          - "--transport.redis.port={{ $.Values.queue.port }}"
          - "--transport.redis.tls={{ $.Values.queue.tls }}"
          {{- if $.Values.queue.servername }}
          - "--transport.redis.servername={{ $.Values.queue.servername }}"
          {{- end }}
          - "--transport.redis.block.duration={{ $.Values.queue.blockDuration }}"
          - "--transport.redis.max.messages={{ $.Values.queue.batchSize }}"
          - "--transport.redis.execution.pool={{ $.Values.queue.executionPool }}"
          - "--transport.redis.pool.max={{ $.Values.queue.pool.max }}"
          - "--transport.redis.pool.min={{ $.Values.queue.pool.min }}"
          {{- if $.Values.queue.timeout.dial }}
          - "--transport.redis.timeout.dial={{ $.Values.queue.timeout.dial }}"
          {{- end }}
          {{- if $.Values.queue.timeout.read }}
          - "--transport.redis.timeout.read={{ $.Values.queue.timeout.read }}"
          {{- end }}
          {{- if $.Values.queue.timeout.write }}
          - "--transport.redis.timeout.write={{ $.Values.queue.timeout.write }}"
          {{- end }}
          {{- if $.Values.queue.timeout.pool }}
          - "--transport.redis.timeout.pool={{ $.Values.queue.timeout.pool }}"
          {{- end }}
          # Redis store
          - "--store.redis.host={{ $.Values.kvstore.host }}"
          - "--store.redis.port={{ $.Values.kvstore.port }}"
          - "--store.redis.tls={{ $.Values.kvstore.tls }}"
          {{- if $.Values.kvstore.servername }}
          - "--store.redis.servername={{ $.Values.kvstore.servername }}"
          {{- end }}
          - "--store.redis.pool.max={{ $.Values.kvstore.pool.max }}"
          - "--store.redis.pool.min={{ $.Values.kvstore.pool.min }}"
          {{- if $.Values.kvstore.timeout.dial }}
          - "--store.redis.timeout.dial={{ $.Values.kvstore.timeout.dial }}"
          {{- end }}
          {{- if $.Values.kvstore.timeout.read }}
          - "--store.redis.timeout.read={{ $.Values.kvstore.timeout.read }}"
          {{- end }}
          {{- if $.Values.kvstore.timeout.write }}
          - "--store.redis.timeout.write={{ $.Values.kvstore.timeout.write }}"
          {{- end }}
          {{- if $.Values.kvstore.timeout.pool }}
          - "--store.redis.timeout.pool={{ $.Values.kvstore.timeout.pool }}"
          {{- end }}
          # Observability
          {{- if $.Values.observability.tracing.url }}
          - "--otel.collector.http.url={{ $.Values.observability.tracing.url }}"
          {{- end }}
          {{- if $.Values.observability.tracing.batchTimeout }}
          - "--otel.batcher.batch.timeout={{ $.Values.observability.tracing.batchTimeout }}"
          {{- end }}
          {{- if $.Values.observability.tracing.exportTimeout }}
          - "--otel.batcher.export.timeout={{ $.Values.observability.tracing.exportTimeout }}"
          {{- end }}
          {{- if $.Values.observability.tracing.maxExportBatchSize }}
          - "--otel.batcher.export.batch.max={{ $.Values.observability.tracing.maxExportBatchSize }}"
          {{- end }}
          {{- if $.Values.observability.tracing.maxQueueSize }}
          - "--otel.batcher.export.queue.max={{ $.Values.observability.tracing.maxQueueSize }}"
          {{- end }}
          # Extra args from values
          {{- with $.Values.taskManager.extraArgs }}
          {{- toYaml . | nindent 10 }}
          {{- end }}
          {{- with ($v.taskManager).extraArgs }}
          {{- toYaml . | nindent 10 }}
          {{- end }}
          env:
          {{- if $.Values.queue.password }}
          - name: SUPERBLOCKS_WORKER_SANDBOX_TRANSPORT_REDIS_PASSWORD
            valueFrom:
              secretKeyRef:
                name: worker-queue-redis-token
                key: token
          {{- end }}
          {{- if $.Values.kvstore.password }}
          - name: SUPERBLOCKS_WORKER_SANDBOX_STORE_REDIS_PASSWORD
            valueFrom:
              secretKeyRef:
                name: worker-kvstore-redis-token
                key: token
          {{- end }}
          {{- if $.Values.superblocks.key }}
          - name: SUPERBLOCKS_WORKER_SANDBOX_SUPERBLOCKS_KEY
            valueFrom:
              secretKeyRef:
                name: worker-agent-key
                key: key
          {{- end }}
          {{- with ($v.taskManager).extraEnv }}
          {{- toYaml . | nindent 10 }}
          {{- end }}

        {{- with $v.nodeSelector | default $.Values.nodeSelector }}
        nodeSelector:
          {{- toYaml . | nindent 10 }}
        {{- end }}
        {{- with $v.affinity | default $.Values.affinity }}
        affinity:
          {{- toYaml . | nindent 10 }}
        {{- end }}
        {{- with $v.tolerations | default $.Values.tolerations }}
        tolerations:
          {{- toYaml . | nindent 10 }}
        {{- end }}
        {{- if $.Values.smokescreen.enabled }}
        volumes:
        - name: smokescreen-config
          configMap:
            name: smokescreen-config
        {{- if $.Values.smokescreen.mtls.enabled }}
        - name: smokescreen-server-cert
          secret:
            secretName: smokescreen-server-cert
        - name: smokescreen-ca-cert
          secret:
            secretName: smokescreen-ca-secret
            items:
            - key: ca.crt
              path: ca.crt
        - name: task-manager-cert
          secret:
            secretName: task-manager-cert
        - name: {{ $language }}-sandbox-cert
          secret:
            secretName: {{ $language }}-sandbox-cert
        {{- end }}
        {{- end }}
{{- end }}
